import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import StandardScaler
import pickle
import json
import re
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# Kanana ìž„ë² ë”© ëª¨ë¸ (Hugging Face)
try:
    from transformers import AutoTokenizer, AutoModel
    import torch
    KANANA_AVAILABLE = True
except ImportError:
    KANANA_AVAILABLE = False
    print("âš ï¸ transformers ë˜ëŠ” torchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. TF-IDFë¥¼ ëŒ€ì‹  ì‚¬ìš©í•©ë‹ˆë‹¤.")
    print("Kanana ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´: pip install transformers torch")

class KananaEmbeddingModel:
    def __init__(self, model_name="kakaocorp/kanana-nano-2.1b-embedding", use_kanana=True):
        self.model_name = model_name
        self.use_kanana = use_kanana and KANANA_AVAILABLE
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if KANANA_AVAILABLE else None
        
        if self.use_kanana:
            try:
                print(f"ðŸš€ Kanana ìž„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘... ({model_name})")
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.model = AutoModel.from_pretrained(model_name)
                self.model.to(self.device)
                self.model.eval()
                print(f"âœ… Kanana ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Device: {self.device})")
            except Exception as e:
                print(f"âŒ Kanana ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("ðŸ”„ TF-IDFë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
                self.use_kanana = False
        
        # Fallback to TF-IDF if Kanana is not available
        if not self.use_kanana:
            self.tfidf_vectorizer = TfidfVectorizer(
                max_features=5000,
                stop_words='english',
                ngram_range=(1, 2),
                min_df=2,
                max_df=0.95
            )
            self.svd = TruncatedSVD(n_components=768, random_state=42)  # Kananaì™€ ë¹„ìŠ·í•œ ì°¨ì›
    
    def encode_texts(self, texts, batch_size=32):
        """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ìž„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        if self.use_kanana:
            return self._encode_with_kanana(texts, batch_size)
        else:
            return self._encode_with_tfidf(texts)
    
    def _encode_with_kanana(self, texts, batch_size=32):
        """Kanana ëª¨ë¸ë¡œ ìž„ë² ë”© ìƒì„±"""
        embeddings = []
        
        with torch.no_grad():
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                
                # í† í°í™”
                inputs = self.tokenizer(
                    batch_texts, 
                    padding=True, 
                    truncation=True, 
                    max_length=512,
                    return_tensors="pt"
                ).to(self.device)
                
                # ìž„ë² ë”© ìƒì„±
                outputs = self.model(**inputs)
                
                # Mean pooling (CLS í† í° ëŒ€ì‹  í‰ê·  í’€ë§ ì‚¬ìš©)
                attention_mask = inputs['attention_mask']
                last_hidden_states = outputs.last_hidden_state
                
                # ë§ˆìŠ¤í¬ë¥¼ ê³ ë ¤í•œ í‰ê·  í’€ë§
                masked_embeddings = last_hidden_states * attention_mask.unsqueeze(-1)
                sum_embeddings = masked_embeddings.sum(dim=1)
                sum_mask = attention_mask.sum(dim=1, keepdim=True)
                mean_embeddings = sum_embeddings / sum_mask
                
                embeddings.extend(mean_embeddings.cpu().numpy())
        
        return np.array(embeddings)
    
    def _encode_with_tfidf(self, texts):
        """TF-IDF + SVDë¡œ ìž„ë² ë”© ìƒì„± (fallback)"""
        tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)
        embeddings = self.svd.fit_transform(tfidf_matrix.toarray())
        
        # L2 ì •ê·œí™”
        from sklearn.preprocessing import normalize
        embeddings = normalize(embeddings, norm='l2')
        
        return embeddings
    
    def compute_similarity(self, embeddings1, embeddings2=None):
        """ìž„ë² ë”© ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        if embeddings2 is None:
            embeddings2 = embeddings1
        
        return cosine_similarity(embeddings1, embeddings2)

class AnimeRecommendationSystem:
    def __init__(self, use_kanana=True):
        self.anime_data = None
        self.content_features = None
        self.collaborative_features = None
        self.user_profiles = {}
        self.content_similarity_matrix = None
        self.svd_model = None
        
        # Kanana ìž„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embedding_model = KananaEmbeddingModel(use_kanana=use_kanana)
        self.use_kanana = self.embedding_model.use_kanana
        
        # TF-IDFëŠ” fallbackìš©ìœ¼ë¡œë§Œ ì‚¬ìš©
        if not self.use_kanana:
            self.tfidf_vectorizer = None
        
    def load_data(self, csv_path):
        """CSV ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print("ë°ì´í„° ë¡œë”© ì¤‘...")
        self.anime_data = pd.read_csv(csv_path, encoding='utf-8')
        print(f"ì´ {len(self.anime_data)}ê°œì˜ ì• ë‹ˆë©”ì´ì…˜ ë°ì´í„° ë¡œë“œë¨")
        
        # ë°ì´í„° í´ë¦¬ë‹
        self._clean_data()
        print("ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ")
        
    def _clean_data(self):
        """ë°ì´í„° ì „ì²˜ë¦¬ ë° í´ë¦¬ë‹"""
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        self.anime_data['genres'] = self.anime_data['genres'].fillna('')
        self.anime_data['tags'] = self.anime_data['tags'].fillna('')
        self.anime_data['synopsis'] = self.anime_data['synopsis'].fillna('')
        self.anime_data['year'] = pd.to_numeric(self.anime_data['year'], errors='coerce')
        self.anime_data['duration'] = pd.to_numeric(self.anime_data['duration'], errors='coerce')
        
        # í…ìŠ¤íŠ¸ ì •ê·œí™”
        self.anime_data['combined_features'] = (
            self.anime_data['title_korean'].fillna('') + ' ' +
            self.anime_data['title_japanese'].fillna('') + ' ' +
            self.anime_data['title_english'].fillna('') + ' ' +
            self.anime_data['genres'] + ' ' +
            self.anime_data['tags'] + ' ' +
            self.anime_data['synopsis']
        )
        
    def build_content_features(self):
        """ì½˜í…ì¸  ê¸°ë°˜ íŠ¹ì„± ë²¡í„° ìƒì„± (Kanana ìž„ë² ë”© ì‚¬ìš©)"""
        print("ì½˜í…ì¸  ê¸°ë°˜ íŠ¹ì„± ë²¡í„° ìƒì„± ì¤‘...")
        
        if self.use_kanana:
            print("ðŸ§  Kanana ìž„ë² ë”©ìœ¼ë¡œ ë²¡í„° ìƒì„± ì¤‘...")
            
            # í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
            texts = self.anime_data['combined_features'].tolist()
            
            # Kanana ëª¨ë¸ë¡œ ìž„ë² ë”© ìƒì„±
            self.content_features = self.embedding_model.encode_texts(texts, batch_size=16)
            
            print(f"âœ… Kanana ìž„ë² ë”© ì™„ë£Œ - í¬ê¸°: {self.content_features.shape}")
            
        else:
            print("ðŸ“ TF-IDF ë²¡í„°í™” ì‚¬ìš© ì¤‘...")
            
            # TF-IDF ë²¡í„°í™” (fallback)
            from sklearn.feature_extraction.text import TfidfVectorizer
            self.tfidf_vectorizer = TfidfVectorizer(
                max_features=5000,
                stop_words='english',
                ngram_range=(1, 2),
                min_df=2,
                max_df=0.95
            )
            
            tfidf_features = self.tfidf_vectorizer.fit_transform(
                self.anime_data['combined_features']
            )
            
            # SVDë¡œ ì°¨ì› ì¶•ì†Œ
            from sklearn.decomposition import TruncatedSVD
            svd = TruncatedSVD(n_components=768, random_state=42)
            self.content_features = svd.fit_transform(tfidf_features.toarray())
            
            # L2 ì •ê·œí™”
            from sklearn.preprocessing import normalize
            self.content_features = normalize(self.content_features, norm='l2')
            
            print(f"âœ… TF-IDF + SVD ì™„ë£Œ - í¬ê¸°: {self.content_features.shape}")
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
        print("ðŸ”„ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚° ì¤‘...")
        self.content_similarity_matrix = self.embedding_model.compute_similarity(self.content_features)
        print(f"âœ… ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ì™„ë£Œ: {self.content_similarity_matrix.shape}")
        
    def create_user_profile(self, user_id, watched_anime_ids, ratings=None):
        """ìœ ì € ì‹œì²­ ì´ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡œí•„ ìƒì„±"""
        if ratings is None:
            ratings = [5.0] * len(watched_anime_ids)
        
        user_profile = {
            'user_id': user_id,
            'watched_anime': watched_anime_ids,
            'ratings': ratings,
            'preferences': self._analyze_user_preferences(watched_anime_ids, ratings)
        }
        
        self.user_profiles[user_id] = user_profile
        return user_profile
    
    def _analyze_user_preferences(self, watched_anime_ids, ratings):
        """ìœ ì € ì·¨í–¥ ë¶„ì„"""
        watched_data = self.anime_data[self.anime_data['id'].isin(watched_anime_ids)]
        
        # ìž¥ë¥´ ì„ í˜¸ë„
        genre_preferences = {}
        tag_preferences = {}
        
        for idx, anime in watched_data.iterrows():
            rating = ratings[watched_anime_ids.index(anime['id'])] if anime['id'] in watched_anime_ids else 3.0
            
            # ìž¥ë¥´ ë¶„ì„
            genres = str(anime['genres']).split('|') if pd.notna(anime['genres']) else []
            for genre in genres:
                if genre.strip():
                    genre_preferences[genre.strip()] = genre_preferences.get(genre.strip(), 0) + rating
            
            # íƒœê·¸ ë¶„ì„
            tags = str(anime['tags']).split('|') if pd.notna(anime['tags']) else []
            for tag in tags:
                if tag.strip():
                    tag_preferences[tag.strip()] = tag_preferences.get(tag.strip(), 0) + rating
        
        return {
            'genre_preferences': dict(sorted(genre_preferences.items(), key=lambda x: x[1], reverse=True)),
            'tag_preferences': dict(sorted(tag_preferences.items(), key=lambda x: x[1], reverse=True)),
            'avg_rating': np.mean(ratings),
            'preferred_years': watched_data['year'].dropna().tolist(),
            'preferred_formats': watched_data['format'].dropna().tolist()
        }
    
    def content_based_recommend(self, user_id, n_recommendations=10):
        """ì½˜í…ì¸  ê¸°ë°˜ ì¶”ì²œ (Kanana ìž„ë² ë”© ì‚¬ìš©)"""
        if user_id not in self.user_profiles:
            return []
        
        user_profile = self.user_profiles[user_id]
        watched_indices = [self.anime_data[self.anime_data['id'] == anime_id].index[0] 
                          for anime_id in user_profile['watched_anime']
                          if not self.anime_data[self.anime_data['id'] == anime_id].empty]
        
        if not watched_indices:
            return []
        
        # ì‹œì²­í•œ ì• ë‹ˆë©”ì´ì…˜ë“¤ì˜ í‰ê·  ìž„ë² ë”© ë²¡í„° ê³„ì‚°
        if self.use_kanana:
            # Kanana ìž„ë² ë”©ì€ ì´ë¯¸ numpy array
            user_vector = np.mean(self.content_features[watched_indices], axis=0)
            
            # ëª¨ë“  ì• ë‹ˆë©”ì´ì…˜ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = self.embedding_model.compute_similarity([user_vector], self.content_features)[0]
        else:
            # TF-IDF + SVD ë²¡í„°
            user_vector = np.mean(self.content_features[watched_indices], axis=0)
            similarities = cosine_similarity([user_vector], self.content_features)[0]
        
        # ì´ë¯¸ ì‹œì²­í•œ ì• ë‹ˆë©”ì´ì…˜ ì œì™¸
        for idx in watched_indices:
            similarities[idx] = -1
        
        # ìƒìœ„ Nê°œ ì¶”ì²œ
        top_indices = similarities.argsort()[-n_recommendations:][::-1]
        
        recommendations = []
        for idx in top_indices:
            anime = self.anime_data.iloc[idx]
            recommendations.append({
                'id': anime['id'],
                'title': anime['title_korean'] or anime['title_japanese'] or anime['title_english'],
                'similarity_score': float(similarities[idx]),  # numpy floatì„ Python floatë¡œ ë³€í™˜
                'genres': anime['genres'],
                'year': anime['year'],
                'synopsis': anime['synopsis'][:200] + "..." if len(str(anime['synopsis'])) > 200 else anime['synopsis']
            })
        
        return recommendations
    
    def item_based_collaborative_recommend(self, user_id, n_recommendations=10):
        """ì•„ì´í…œ ê¸°ë°˜ í˜‘ì—… í•„í„°ë§ ì¶”ì²œ"""
        if user_id not in self.user_profiles:
            return []
        
        user_profile = self.user_profiles[user_id]
        watched_indices = [self.anime_data[self.anime_data['id'] == anime_id].index[0] 
                          for anime_id in user_profile['watched_anime']
                          if not self.anime_data[self.anime_data['id'] == anime_id].empty]
        
        if not watched_indices:
            return []
        
        # ì‹œì²­í•œ ì• ë‹ˆë©”ì´ì…˜ë“¤ê³¼ ìœ ì‚¬í•œ ì• ë‹ˆë©”ì´ì…˜ ì°¾ê¸°
        item_similarities = {}
        for watched_idx in watched_indices:
            similar_items = self.content_similarity_matrix[watched_idx]
            for i, similarity in enumerate(similar_items):
                if i not in watched_indices and similarity > 0.1:
                    item_similarities[i] = item_similarities.get(i, 0) + similarity
        
        # ìƒìœ„ Nê°œ ì¶”ì²œ
        top_items = sorted(item_similarities.items(), key=lambda x: x[1], reverse=True)[:n_recommendations]
        
        recommendations = []
        for idx, score in top_items:
            anime = self.anime_data.iloc[idx]
            recommendations.append({
                'id': anime['id'],
                'title': anime['title_korean'] or anime['title_japanese'] or anime['title_english'],
                'similarity_score': score,
                'genres': anime['genres'],
                'year': anime['year'],
                'synopsis': anime['synopsis'][:200] + "..." if len(str(anime['synopsis'])) > 200 else anime['synopsis']
            })
        
        return recommendations
    
    def hybrid_recommend(self, user_id, n_recommendations=10, content_weight=0.6, collaborative_weight=0.4):
        """í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ (ì½˜í…ì¸  ê¸°ë°˜ + í˜‘ì—… í•„í„°ë§)"""
        content_recs = self.content_based_recommend(user_id, n_recommendations*2)
        collab_recs = self.item_based_collaborative_recommend(user_id, n_recommendations*2)
        
        # ì¶”ì²œ ì ìˆ˜ ì •ê·œí™” ë° ê²°í•©
        combined_scores = {}
        
        for rec in content_recs:
            anime_id = rec['id']
            combined_scores[anime_id] = {
                'anime': rec,
                'content_score': rec['similarity_score'] * content_weight,
                'collab_score': 0
            }
        
        for rec in collab_recs:
            anime_id = rec['id']
            if anime_id in combined_scores:
                combined_scores[anime_id]['collab_score'] = rec['similarity_score'] * collaborative_weight
            else:
                combined_scores[anime_id] = {
                    'anime': rec,
                    'content_score': 0,
                    'collab_score': rec['similarity_score'] * collaborative_weight
                }
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        for anime_id in combined_scores:
            total_score = (combined_scores[anime_id]['content_score'] + 
                          combined_scores[anime_id]['collab_score'])
            combined_scores[anime_id]['final_score'] = total_score
        
        # ìƒìœ„ Nê°œ ë°˜í™˜
        sorted_recommendations = sorted(
            combined_scores.items(), 
            key=lambda x: x[1]['final_score'], 
            reverse=True
        )[:n_recommendations]
        
        final_recommendations = []
        for anime_id, scores in sorted_recommendations:
            rec = scores['anime'].copy()
            rec['final_score'] = scores['final_score']
            rec['content_score'] = scores['content_score']
            rec['collab_score'] = scores['collab_score']
            final_recommendations.append(rec)
        
        return final_recommendations
    
    def get_trending_anime(self, year_range=(2020, 2025), n_recommendations=10):
        """íŠ¸ë Œë”© ì• ë‹ˆë©”ì´ì…˜ ì¶”ì²œ"""
        recent_anime = self.anime_data[
            (self.anime_data['year'] >= year_range[0]) & 
            (self.anime_data['year'] <= year_range[1])
        ].copy()
        
        # ìž¥ë¥´ ë‹¤ì–‘ì„±ê³¼ ë…„ë„ë¥¼ ê³ ë ¤í•œ ì ìˆ˜ ê³„ì‚°
        recent_anime['trend_score'] = (
            recent_anime['year'] / recent_anime['year'].max() * 0.5 +
            recent_anime['genres'].str.count('\|') * 0.1 +
            recent_anime['tags'].str.count('\|') * 0.1 +
            0.3  # ê¸°ë³¸ ì ìˆ˜
        )
        
        trending = recent_anime.nlargest(n_recommendations, 'trend_score')
        
        recommendations = []
        for idx, anime in trending.iterrows():
            recommendations.append({
                'id': anime['id'],
                'title': anime['title_korean'] or anime['title_japanese'] or anime['title_english'],
                'trend_score': anime['trend_score'],
                'genres': anime['genres'],
                'year': anime['year'],
                'synopsis': anime['synopsis'][:200] + "..." if len(str(anime['synopsis'])) > 200 else anime['synopsis']
            })
        
        return recommendations
    
    def save_model(self, filepath):
        """ëª¨ë¸ ì €ìž¥ (Kanna ëª¨ë¸ ì œì™¸)"""
        model_data = {
            'user_profiles': self.user_profiles,
            'content_features': self.content_features,
            'content_similarity_matrix': self.content_similarity_matrix,
            'use_kanana': self.use_kanana,
            'anime_data_columns': self.anime_data.columns.tolist() if self.anime_data is not None else None
        }
        
        # TF-IDF ëª¨ë¸ì´ ìžˆìœ¼ë©´ ì €ìž¥
        if not self.use_kanana and hasattr(self.embedding_model, 'tfidf_vectorizer'):
            model_data['tfidf_vectorizer'] = self.embedding_model.tfidf_vectorizer
            model_data['svd_model'] = self.embedding_model.svd
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"ëª¨ë¸ì´ {filepath}ì— ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("âš ï¸ Kanana ëª¨ë¸ì€ ë³„ë„ë¡œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤.")
    
    def load_model(self, filepath):
        """ëª¨ë¸ ë¡œë“œ"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.user_profiles = model_data['user_profiles']
        self.content_features = model_data['content_features']
        self.content_similarity_matrix = model_data['content_similarity_matrix']
        
        # Kanana ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ ë³µì›
        saved_use_kanana = model_data.get('use_kanana', False)
        if saved_use_kanana and not self.use_kanana:
            print("âš ï¸ ì €ìž¥ëœ ëª¨ë¸ì€ Kananaë¥¼ ì‚¬ìš©í–ˆì§€ë§Œ í˜„ìž¬ Kananaê°€ ë¹„í™œì„±í™”ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.")
        
        # TF-IDF ëª¨ë¸ ë³µì› (í•„ìš”í•œ ê²½ìš°)
        if not self.use_kanana:
            if 'tfidf_vectorizer' in model_data:
                self.embedding_model.tfidf_vectorizer = model_data['tfidf_vectorizer']
            if 'svd_model' in model_data:
                self.embedding_model.svd = model_data['svd_model']
        
        print(f"ëª¨ë¸ì´ {filepath}ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        if saved_use_kanana:
            print("âœ… Kanana ëª¨ë¸ ì„¤ì •ì´ ë³µì›ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def evaluate_recommendations(self, test_users, test_data, k=10):
        """ì¶”ì²œ ì„±ëŠ¥ í‰ê°€"""
        precision_scores = []
        recall_scores = []
        
        for user_id, actual_anime in test_data.items():
            if user_id in test_users:
                recommendations = self.hybrid_recommend(user_id, k)
                recommended_ids = [rec['id'] for rec in recommendations]
                
                # Precisionê³¼ Recall ê³„ì‚°
                relevant_items = set(actual_anime)
                recommended_items = set(recommended_ids)
                
                if len(recommended_items) > 0:
                    precision = len(relevant_items & recommended_items) / len(recommended_items)
                    precision_scores.append(precision)
                
                if len(relevant_items) > 0:
                    recall = len(relevant_items & recommended_items) / len(relevant_items)
                    recall_scores.append(recall)
        
        avg_precision = np.mean(precision_scores) if precision_scores else 0
        avg_recall = np.mean(recall_scores) if recall_scores else 0
        f1_score = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0
        
        return {
            'precision': avg_precision,
            'recall': avg_recall,
            'f1_score': f1_score
        }